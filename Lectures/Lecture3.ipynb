{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 3: Introduction to Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) Backpropagation:\n",
    "* <u>**Chain rule:**</u> To ease gradient computations, we use computational graph, in which we can apply the chain rule: current_gradient = [upstream gradient] x [local gradient] $$ \\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial q} \\cdot \\frac{\\partial q}{\\partial x}$$\n",
    "* <u>**Sigmoid gate:**</u> $$\\sigma (x) = \\frac{1}{1+e^{-x}} \\rightarrow \\sigma '(x) = (1-\\sigma (x))\\cdot \\sigma (x)$$\n",
    "* <u>**Add gate:**</u> Plays role of gradient distributor. During backpropagation, passes its value to each of its entries (same value). \n",
    "* <u>**Max gate:**</u> Plays role of gradient router. During backpropagation, passes its value to one of its entries (the others get 0). \n",
    "* <u>**Multiplication gate:**</u> Plays role of gradient switcher. During backpropagation, take the upstream gradient and scale it according to the value of the other branch.\n",
    "* <u>**Sum of multiple entries:**</u> $$ \\frac{\\partial f}{\\partial x} = \\sum_i \\frac{\\partial f}{\\partial q_i} \\cdot \\frac{\\partial q_i}{\\partial x} $$\n",
    "* <u>**Vectorized operations:**</u> $\\frac{\\partial f}{\\partial x}$ is now a Jacobian matrix of size $N_{input} \\times N_{input}$ (usually, we process a full minibatch at a time so $ N_{input} = sampleSize \\cdot \\#samples $) $$ \\frac{\\partial L}{\\partial x} = \\frac{\\partial f}{\\partial x} \\cdot \\frac{\\partial L}{\\partial f} $$\n",
    "    - The gradient with respect to a variable should have the same shape as the variable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modularized implementation of forward/backward propagation: (Caffe layers library contains a lot of computational nodes implementations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputationalGraph(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, inputs):\n",
    "        for gate in self.graph.nodes_topologically_sorted():\n",
    "            gate.forward()\n",
    "        return loss\n",
    "    def backward(self):\n",
    "        for gate in reversed(self.graph.nodes_topologically_sorted()):\n",
    "            gate.backward()\n",
    "        return input_gradients\n",
    "\n",
    "class MultiplyGate(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, x, y):\n",
    "        z = x*y\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        return z\n",
    "    def backward(self, dz):\n",
    "        dx = self.y * dz # dL/dx\n",
    "        dy = self.x * dz # dL/dy\n",
    "        return [dx,dy]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Neural Networks:\n",
    "* <u>**1 layer:**</u> Linear layer: $ f = Wx$\n",
    "* <u>**2 layers:**</u> Add non-linearity to avoid collapsing into linear layer: $ f = W_2 \\cdot max(0, W_1 \\cdot x)$\n",
    "* <u>**3 layers:**</u> Expansion of the idea: $ f = W_3 \\cdot max(0, W_2 \\cdot max(0, W_1 \\cdot x))$\n",
    "* <u>**Perceptron:**</u> General idea of neuron (action potential): $ output_{neuron} = f(\\sum_i w_i x_i + b_i)$ (while $f$ is an activation function)\n",
    "* <u>**Activation functions:**</u> Sigmoid, ReLU, tanh, Leaky ReLU, Maxout, ELU..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaolin-env-kernel",
   "language": "python",
   "name": "kaolin_env"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
