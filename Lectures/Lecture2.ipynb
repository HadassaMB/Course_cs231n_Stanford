{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture 2: Loss functions and optimization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) Loss function:\n",
    "* <u>**General formulation:**</u> $$ L=\\frac{1}{N} \\sum_i L_i(f(x_i,W),y_i)$$\n",
    "* <u>**Multiclass SVM loss:**</u> for each sample $\\{x_i,y_i\\}$, compute hinge loss ($s_k$ is the score for class $k$, for example Wx+b in the case of linear classifier). Meaning, the loss is low if the true label score is much higher than all the other label scores (by some safety margin - here 1, choice does not really matter since the weights will be scaled accordingly): \n",
    "            $$L_i = \\sum_{j\\neq y_i} {max(0, s_j - s_{y_i} +1)}$$\n",
    "* <u>**Regularization:**</u> \n",
    "    - if we only consider data loss, we face overfitting problem (the classifier will try to match the data as much as possible without good generalization for unseen samples - the test set). Regularization aims to simplify the model for a better generalization ($\\lambda$ is a hyperparameter).\n",
    "            $$ L(W)=\\frac{1}{N} \\sum_i L_i(f(x_i,W),y_i) + \\lambda R(W)$$\n",
    "    - Examples: \n",
    "        - L1: $R(W) = \\sum_k \\sum_l W_{k,l}^2$\n",
    "        - L2: $R(W) = \\sum_k \\sum_l |W_{k,l}|$ (also: MAP inference using Gaussian prior on W)\n",
    "        - Elastic net (L1 + L2): $R(W) = \\sum_k \\sum_l \\beta W_{k,l}^2 + |W_{k,l}|$\n",
    "        - Batch normalization, dropout, max norm, stochastic depth\n",
    "    - In a nutshell, penalizes the complexity of the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Softmax classifier (Multinomial Logistic Regression):\n",
    "* <u>**Cross entropy loss based:**</u> \n",
    "    - We want to minimize:\n",
    "            $$ L_i = -log(P(Y=y_i|X=x_i)) $$\n",
    "    Assuming $ s = f(x_i,W) $ and:\n",
    "            $$ P(Y=k|X=x_i) = \\frac{e^{s_k}}{\\sum_j e^{s_j}} $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3) Optimization:\n",
    "* <u>**Numerical gradient:**</u> slow, approximate, but easy to write -> good for debugging (gradient check).\n",
    "    * Consists of numerically computing the gradient for each entry of W.\n",
    "* <u>**Analytical gradient:**</u> fast, exact, but error prone -> always use it.\n",
    "    * Consists of analytically computing $\\nabla_W L$\n",
    "* <u>**Gradient descent:**</u> $W^{(t+1)} = W^{(t)} - \\eta \\cdot \\nabla_W L$ \n",
    "    * $\\eta$ is the step size (usually most important hyperparameter).\n",
    "    * $\\nabla_W L$ is supposed to point out to the direction of a minimum.\n",
    "* <u>**Stochastic Gradient Descent (SGD):**</u>\n",
    "    * Computing the full $\\nabla_W L(W)=\\frac{1}{N} \\sum_i \\nabla_W L_i(f(x_i,W),y_i) + \\lambda \\nabla_W R(W)$ is computationally expensive.\n",
    "    * Instead, at each iteration, we sample a minibatch of samples (typically, N=32,64,128,...) and compute $\\nabla_W L(W)$ over it, and update the weights accordingly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaolin-env-kernel",
   "language": "python",
   "name": "kaolin_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
